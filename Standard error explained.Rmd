---
title: 'A Gentle Introduction to Standard Error and Bootstrapping'
output: html_document
---

Many of the points and code in this blog post are adapted from the excellent [Statistical Inference](https://www.coursera.org/course/statinference) unit on Coursera by Brian Caffo, Jeff Leek and Roger Peng. This course gives a far more comprehensive coverage of this material and is highly recommended.

## What is the standard error of an estimate?

Suppose you want to assess the average number of page views a website has per day. How do you measure this? Well, the most logical thing to do is to take a representative sample e.g., a sequence of 30 days, record the number of page views, and take their mean. However, how do you know if this is a good estimate the true daily average number of page views? How do you know if a sample of this size is big enough?

The **standard error** of any statistic is a measure of the amount of error in the sample statistic with respect to the true population value. In this case, the standard error of the mean would be an estimate of how accurately the daily page views of our sample reflects the true population mean. The smaller the standard error of the mean, the better the sample estimate reflects the true population value.

## Why should I care about the standard error?

It is tempting, once you get into applying statistics to a sample, to forget you are dealing with a representation of a population, rather than the population itself. As such, all statistics you derive using your sample are just estimates of the true population parameters you are hoping to talk about (although hopefully pretty accurate ones). It is therefore important that you have some idea how reliable these estimates are before you start making inferences from them.

## How is the standard error calculated?

One way of assessing the standard error of the mean is to sample repeated from the population, calculate the mean for each sample, and plot the distribution of these means. Each sample is expected to be a different representation of the population, with different estimates of the median in each sample. 

Imagine if you were working for a company that wanted to know how many page views their website received. One thing you could do is take a large number, say 1,000, samples of 30 day periods each. Instead of doing this (because it would take about 82 years...), let's pretend we already know the population rate (220 page views per day, or a Poisson distribution with $\lambda$ = 220/day) and use this to simulate this result in R:

```{r, message = FALSE}
rm(list = ls())         # Clear the workspace
set.seed(567)           # Set seed to replicate random variable generation
mn_vector <- NULL      # Generate the median of each sample and store in a vector
sample_frame <- data.frame(row.names = seq(from = 1, to = 30, by = 1))
for (i in 1 : 1000) {
    s <- rpois(30, lambda = 220)
    sample_frame <- cbind(sample_frame, s)
    mn_vector <- c(mn_vector, mean(s))
}
names(sample_frame) <- paste0("n", seq(from = 1, to = 1000, by = 1))
```

For example, your first sample has a mean rate of `r round(mn_vector[1])` page views per day, and the second sample has a mean rate of `r round(mn_vector[2])` page views per day. Their distributions are demonstrated below:

```{r, message = FALSE, fig.width = 13}
require(ggplot2); require(gridExtra)
barfill <- "#6baed6"
barlines <- "#2171b5"
line1 <- "black"
line2 <- "#FF0000"
g1 <- ggplot(data=sample_frame, aes(sample_frame$n1)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 4, fill = barfill, colour = barlines) +
        xlab("Sample heights") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 1") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold"))

mean1 <- data.frame(Means="Theoretical mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(sample_frame$n1))
means <- rbind(mean1, mean2)
                
g1 <- g1 + geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
      scale_color_manual(values=c("Theoretical mean" = line1, "Sample mean" = line2)) +
      theme(legend.position="none")
            
g2 <- ggplot(data=sample_frame, aes(sample_frame$n2)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 4, fill = barfill, colour = barlines) +
        xlab("Sample heights") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 2") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold"))

mean1 <- data.frame(Means="Theoretical mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(sample_frame$n2))
means <- rbind(mean1, mean2)
                
g2 <- g2 + geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
      scale_color_manual(values=c("Theoretical mean" = line1, "Sample mean" = line2)) +
      theme(legend.position="none")

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

As you can see, the sample mean (in red) almost completely mirrors the population mean (in black) in sample 1, but there is quite a bit of difference between the two values in sample two.

It turns out that the distribution of the mean of the samples is approximately normally distributed. This is the **Central Limit Theorem (CLT)**, wherein the distribution of means of repeated samples of independently and identically distributed (or _iid_) observations becomes normal at a sufficiently large sample size.

This can be seen in the histogram of the means of each sample:

```{r, message = FALSE, fig.width=11}
g3 <- ggplot(data=as.data.frame(mn_vector), aes(mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each sample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Samples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) + 
        geom_line(aes(y = ..density.., colour = "Empirical"), stat = "density") + 
        stat_function(fun = dnorm, aes(colour = "Normal"), 
                    arg = list(mean = 220, sd = sd(mn_vector))) + 
        scale_colour_manual(name = "Density", values = c(line1, line2))

print(g3)
```

The mean of this distribution should be a pretty close estimate of the sample mean - and it is in this case, equalling `r round(mean(mn_vector))`. If we take the standard deviation of this distribution, we get the standard error of the mean. In other words, because these means are normally distributed, $\pm$1 standard error around this distribution mean represents the range that 68% of the sample means falls within, $\pm$2 standard errors represents the range that 95% of the sample means falls within, and so on.

In our case, taking a sample of 30 days gives us a pretty accurate assessment of the sample mean, with 68% of our samples giving an estimate falling between `r round(mean(mn_vector) - sd(mn_vector))` and `r round(mean(mn_vector) + sd(mn_vector))`, and 95% of our values falling within `r round(mean(mn_vector) - 2 * sd(mn_vector))` and `r round(mean(mn_vector) + 2 * sd(mn_vector))`. This is a pretty tight band around our population mean of 220 page views per day, indicating that a sample of 30 gives a pretty good estimate of the mean.

```{r, fig.width=11}
g4 <- ggplot(data=as.data.frame(mn_vector), aes(mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each sample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Samples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold"))

sem1 <- data.frame(SEMs="+/- 1 SEM", 
                   vals = c(mean(mn_vector) - sd(mn_vector), 
                            mean(mn_vector) + sd(mn_vector)))
sem2 <- data.frame(SEMs="+/- 2 SEMs", 
                    vals = c(mean(mn_vector) - 2 * sd(mn_vector), 
                            mean(mn_vector) + 2 * sd(mn_vector)))
sems <- rbind(sem1, sem2)
                
g4 <- g4 + geom_vline(data=sems, aes(xintercept=vals, linetype = SEMs, 
                            colour = SEMs), size = 1, show_guide = TRUE) + 
      scale_color_manual(values=c("+/- 1 SEM" = line1, 
                                  "+/- 2 SEMs" = line1))

print(g4)
```

## What if I only have one sample?

Obviously the above is more of a thought exercise than anything. In real life, we don't have the population distribution, and we also usually don't have the luxury of taking multiple samples. In what case, what do we do? A way is to _bootstrap_ using our population sample. Bootstrapping involves basically the same steps as above, except instead of sampling from the population distribution, we sample from our sample distribution.

Let's assume we've taken a sample of page views over a 30 day sample:
```{r}
set.seed(567)
sample <- rpois(30, lambda = 220)

g5 <- ggplot(data=as.data.frame(sample), aes(sample)) + 
        geom_histogram(aes(y = ..density..), binwidth = 6, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Number of page views per day") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Sample distribution of page views over 30 days") + 
        theme(plot.title = element_text(lineheight=.8, face="bold"))

print(g5)
```

The mean of this distribution is `r round(mean(sample))` page views per day, which is pretty much the population mean (which we would expect given the tight standard error around the mean we saw earlier).

To create a bootstrapped distribution of sample means, all we need to do is take repeated samples from this sample distribution _with replacement_ and take their means. The reason we must do this with replacement is because if we have a sample distribution of size, say 30, and we take a sample from this of size 30, we will just end up with the original sample! Importantly, this process rests on the assumption that our sample is a pretty accurate representation of our sample. We'll cover this point at the end of the blog post.

So let's do this in R. We will take our sample of 30 days of page views and take 1,000 samples from this with replacement.

```{r}
set.seed(567)
r_mn_vector <- NULL
resample_frame <- data.frame(row.names = seq(from = 1, to = 30, by = 1))
for (i in 1 : 1000) {
    s <- sample(sample, 30, replace = TRUE)
    resample_frame <- cbind(resample_frame, s)
    r_mn_vector <- c(r_mn_vector, mean(s))
}

names(resample_frame) <- paste0("n", seq(from = 1, to = 1000, by = 1))
```

As with sampling from the population, we can see we get variability in our estimate of the population mean from sample to sample. For example, the mean in resample 1 is `r round(mean(resample_frame$n1))`, and the mean from resample 2 is `r round(mean(resample_frame$n2))`.

```{r, message = FALSE, fig.width = 13}
g6 <- ggplot(data=resample_frame, aes(resample_frame$n1)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 5, fill = barfill, colour = barlines) +
        xlab("Sample heights") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 1") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold"))

mean1 <- data.frame(Means="Theoretical mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(resample_frame$n1))
means <- rbind(mean1, mean2)
                
g6 <- g6 + geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
      scale_color_manual(values=c("Theoretical mean" = line1, "Sample mean" = line2)) +
      theme(legend.position="none")
            
g7 <- ggplot(data=resample_frame, aes(resample_frame$n2)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 5, fill = barfill, colour = barlines) +
        xlab("Sample heights") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 2") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold"))

mean1 <- data.frame(Means="Theoretical mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(resample_frame$n2))
means <- rbind(mean1, mean2)
                
g7 <- g7 + geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
      scale_color_manual(values=c("Theoretical mean" = line1, "Sample mean" = line2)) +
      theme(legend.position="none")

grid.arrange(g6, g7, nrow = 1, ncol = 2)
```

As with the means of the samples drawn from the population mean, the resamples drawn from the sample mean also approximate the normal distribution:

```{r, fig.width=15}
g8 <- ggplot(data=as.data.frame(r_mn_vector), aes(r_mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each sample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Samples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) + 
        geom_line(aes(y = ..density.., colour = "Empirical"), stat = "density") + 
        stat_function(fun = dnorm, aes(colour = "Normal"), 
                    arg = list(mean = 220, sd = sd(mn_vector))) + 
        scale_colour_manual(name = "Density", values = c(line1, line2))

print(g8)
```

The bootstrapped SEM using 1,000 resamplings was `r round(sd(r_mn_vector), 2)`, which is extremely close to the SEM derived from sampling from the population (`r round(sd(mn_vector), 2)`). This is because the sample we took of daily page views was a good representation of the population.

## Why would I bootstrap instead of simply calculating the SEM?

You might have been thinking, "Why would I bootstrap my SEM? It is pretty simple to calculate the SEM without all of these complicated simulations!" Well, bootstrapping is a great way of calculating information such as standard errors and confidence intervals when your data do not meet the assumptions of the distribution you are working with. For example, you might have outliers that would massively bias your standard error (and also your mean!) if you calculated these parameters directly from the sample rather than bootstrapping them.

```{r}
sample_outlier <- c(sample, 300)
sample_no_outlier <- c(sample, 221)

sem_outlier <- sd(sample_outlier) / sqrt(31)
sem_no_outlier <- sd(sample_no_outlier) / sqrt(31)

set.seed(567)
r_mn_vector <- NULL
for (i in 1 : 10000) {
    s <- sample(sample_outlier, 30, replace = TRUE)
    r_mn_vector <- c(r_mn_vector, mean(s))
}

stat<-function(x,i){mean(x[i])}
boot.out<-boot(data=sample_outlier, statistic=stat, R=1000)
boot.ci(boot.out, conf = 0.68)

stat<-function(x,i){mean(x[i])}
boot.out<-boot(data=sample_no_outlier, statistic=stat, R=1000)
boot.ci(boot.out, conf = 0.68)
```

## How do I know if my sample represents my population?

Choosing a good sample is less about number crunching and more about study design. Depending on the population of interest, it can be a bit tricky. Important elements of representative sampling include:  
1. **Sample size:** The trick to choosing a large enough sample size is making sure you gain sufficient information about the patterns you are trying to describe. For example, if we had only sampled 10 days, would that have been too small to avoid sampling extreme values by chance that threw off our estimate of the mean? If the event we were trying to sample was very rare (e.g., 1 event per 100 days), would sampling 30 days be long enough to assess its mean occurrence? However, the opposite consideration often occurs with sample size, where taking samples that are much larger than is needed is also undesirable as there may be time, monetary or even ethical reasons to limit the number of observations collected. For example, the company that asked us to assess page views would likely be unhappy if we spent 100 days collecting information on mean daily page views when the same question could be reliably answered from 30 days of data collection. There is a whole collection of methods that help you decide on the correct sample size (called **power**) which are beyond the scope of this already long blog post.
2. **Representativeness:** Another consideration is that we must make the data we collect a good reflection of the population we are trying to describe. Say that I tried to collect my data after a new campaign was launched advertising the website. This would not be a representative sample of the usual mean page views per day as page views are likely to be higher at that time.
3. **Measurement:** A final step in gaining a representative sample is remembering there may be a gap between a _concept_ of interest and the _method_ you need to use to measure it. In our example, there is likely to be little gap between what we are trying to measure (number of page views) and the method (a counter of page visitors). However, what if we don't actually care about page views per say, but the popularity of a website? Would it be reasonable to say that websites with higher mean page views have greater popularity?


