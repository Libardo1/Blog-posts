---
title: 'A Gentle Introduction to Standard Error'
output: html_document
---

## What is the standard error of an estimate?

Imagine if you were working for a company that wanted to know how many page views their website received per day, on average. How do you measure this? Well, the most logical thing to do is to pick a sample of days, record the number of page views, and take the mean of these. However, how do you know if this is a good estimate the true average number of page views per day? How do you know if a sample of this size is big enough?

The **standard error** of any statistic is a measure of the amount of error in the sample statistic with respect to the true population value. In this case, we want to assess the **standard error of the mean** (or **SEM**), which is an estimate of how accurately the mean number of daily page views _in our sample_ reflects the _true_ mean number of daily page views. The smaller the standard error of the mean, the better the sample estimate reflects the true population value. There are formulas for calculating the SEM depending on the distribution you are dealing with, but in this blog post I will describe the general principles underlying the SEM.

## Why should I care about the standard error?

It is easy once you get into applying statistics to a sample to forget you are dealing with a representation of a population, rather than the population itself. As such, all statistics you derive using your sample are just estimates of the true population parameters you are hoping to talk about. It is therefore important that you have some idea how reliable these estimates are before you start making inferences from them.

## How is the standard error calculated?

One way of assessing the SEM is to sample repeated from the population, calculate the mean for each sample, and plot the distribution of these means. Each sample is expected to be a different representation of the population, with different estimates of the mean in each sample. 

Let's revisit our problem of assessing the accuracy of the mean number of page views. One thing you could do is take a large number of samples of 30 day periods each, where large means 1,000 or more. Instead of doing this (because it would take about 82 years...), let's pretend we already know the population mean (220 page views per day, or a Poisson distribution with $\lambda$ = 220/day) and use this to simulate this result in R:

```{r, message = FALSE}
# Clear the workspace
rm(list = ls())

# Set seed to replicate random variable generation
set.seed(567)

# Generate the mean of each sample and store in a vector, and store each sample in a dataframe
mn_vector <- NULL
sample_frame <- data.frame(row.names = seq(from = 1, to = 30, by = 1))
for (i in 1 : 1000) {
    s <- rpois(30, lambda = 220)
    sample_frame <- cbind(sample_frame, s)
    mn_vector <- c(mn_vector, mean(s))
}

# Name the columns in the sample dataframe
names(sample_frame) <- paste0("n", seq(from = 1, to = 1000, by = 1))
```

For example, your first sample has a mean rate of `r round(mn_vector[1])` page views per day, and the second sample has a mean rate of `r round(mn_vector[2])` page views per day. Looking at the sample distributions (below), the sample mean almost completely mirrors the population mean in sample 1, but there is quite a bit of difference between the two values in sample 2.

```{r, message = FALSE, echo = FALSE, fig.width = 13}
# Load required packages
require(ggplot2); require(gridExtra)

# Set the colours for the graphs
barfill <- "#6baed6"
barlines <- "#2171b5"
line1 <- "black"
line2 <- "#FF0000"

# Plotting histogram of sample 1
mean1 <- data.frame(Means="Theoretical mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(sample_frame$n1))
means <- rbind(mean1, mean2)

g1 <- ggplot(data=sample_frame, aes(sample_frame$n1)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 4, fill = barfill, colour = barlines) +
        xlab("Sample heights") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 1") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold")) +
        geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("Theoretical mean" = line1, "Sample mean" = line2))
            
# Plotting histogram of sample 2
mean1 <- data.frame(Means="Theoretical mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(sample_frame$n2))
means <- rbind(mean1, mean2)

g2 <- ggplot(data=sample_frame, aes(sample_frame$n2)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 4, fill = barfill, colour = barlines) +
        xlab("Sample heights") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 2") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold")) +
        geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("Theoretical mean" = line1, "Sample mean" = line2))

# Printing histograms
grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

It turns out that the distribution of the mean of the samples is approximately normally distributed. This property is described by the **Central Limit Theorem (CLT)**, wherein the distribution of means of repeated samples of independently and identically distributed (or _iid_) observations becomes normal at a sufficiently large sample size.

This can be seen in the histogram of the means of each sample:

```{r, message = FALSE, echo=FALSE, fig.width=11}
# Plotting histogram of the distribution of sample means
g1 <- ggplot(data=as.data.frame(mn_vector), aes(mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each sample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Samples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) + 
        geom_line(aes(y = ..density.., colour = "Empirical"), stat = "density") + 
        stat_function(fun = dnorm, aes(colour = "Normal"), 
                    arg = list(mean = 220, sd = sd(mn_vector))) + 
        scale_colour_manual(name = "Density", values = c(line1, line2))

print(g1)
```

The mean of this distribution should be a pretty close estimate of the sample mean - and it is, equalling `r round(mean(mn_vector), 2)`. If we take the standard deviation of this distribution, we get the standard error of the mean. Because these means are normally distributed, &plusmn; 1 standard error around the mean of the sample means represents the range that 68% of the sample means fall within, &plusmn; 2 standard errors represents the range that 95% of the sample means fall within, and so on.

In our case, taking a sample of 30 days gives us a pretty accurate assessment of the population mean, with 68% of our samples giving a mean between `r round(mean(mn_vector) - sd(mn_vector), 2)` and `r round(mean(mn_vector) + sd(mn_vector), 2)`, and 95% of our samples giving a mean between `r round(mean(mn_vector) - 2 * sd(mn_vector), 2)` and `r round(mean(mn_vector) + 2 * sd(mn_vector), 2)`. In other words, 68% of the time when we take a sample we will end up with a mean between `r round(mean(mn_vector) - sd(mn_vector), 2)` and `r round(mean(mn_vector) + sd(mn_vector), 2)`, and 95% of the time when we take a sample we will end up with a mean between `r round(mean(mn_vector) - 2 * sd(mn_vector), 2)` and `r round(mean(mn_vector) + 2 * sd(mn_vector), 2)`. This is a pretty tight band around our population mean of 220 page views per day, indicating that a sample of 30 gives a pretty good estimate of the mean.

```{r, echo = FALSE, fig.width=11}
# Plotting a histogram with the +/- 1 and 2 standard error intervals.
sem1 <- data.frame(SEMs="+/- 1 SEM", 
                   vals = c(mean(mn_vector) - sd(mn_vector), mean(mn_vector) + sd(mn_vector)))
sem2 <- data.frame(SEMs="+/- 2 SEMs", 
                    vals = c(mean(mn_vector) - 2 * sd(mn_vector), mean(mn_vector) + 2 * sd(mn_vector)))
sems <- rbind(sem1, sem2)

g1 <- ggplot(data=as.data.frame(mn_vector), aes(mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each sample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Samples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) +
        geom_vline(data=sems, aes(xintercept=vals, linetype = SEMs, 
                            colour = SEMs), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("+/- 1 SEM" = line1, 
                                    "+/- 2 SEMs" = line2))

print(g1)
```

## The take away message

As you can see, the SEM is a useful indication of how likely it is that your sample mean is an accurate reflection of the population value. You can also see it is highly dependent on the size of the sample you choose. While I have demonstrated calculating the SEM for the mean of a Poisson-distributed variable, the same principles apply with any type of distribution.

Much of the points and code in this blog post are adapted from the excellent [Statistical Inference](https://www.coursera.org/course/statinference) unit on Coursera by Brian Caffo, Jeff Leek and Roger Peng. This course gives a far more comprehensive coverage of this material and is highly recommended.

Finally, the full code used to create the figures in this post are located in the [repo on my Github page](https://github.com/t-redactyl/t-redactyl.github.io/tree/master/blog) for this website.