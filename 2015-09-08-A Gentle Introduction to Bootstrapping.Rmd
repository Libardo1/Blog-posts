---
title: A Gentle Introduction to Bootstrapping
date: 2015-09-08
comments: false
tags: Statistics, R, Data Simulations
keywords: rlanguage, bootstrapping, simulations, standard error of the mean, data science
---

In the previous post, I explained the general principles behind the [**standard error of the mean**](https://en.wikipedia.org/wiki/Standard_error) (or **SEM**). The idea underlying the SEM is that if you take repeated samples from the population of interest and take the standard deviation of the means of these samples, this gives you an estimate of the accuracy of the sample mean compared to the population mean. Formulas for the SEM are available for each distribution, and accurately reflect the degree of "fuzziness" in your sample mean **if your data meet the assumptions of that distribution.**

## What if I'm not sure if my data meets the assumptions of a distribution?
You might have a case where the true population distribution of your sample doesn't meet the assumptions of the closest appropriate distribution. For example, you might be interested in describing the mean levels of depression in a population, which is naturally positively skewed (i.e., most people in a population have no or low scores on measures of depression). In this case, using the SEM formula for the normal distribution would be inappropriate due to this skewness. Instead, you can create empirically derived SEMs using a technique called [**bootstrapping.**](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
    
## What is bootstrapping?
Bootstrapping involves basically the same principle as calculating the SEM I described in the last post, except instead of **sampling from the population distribution**, we **esample from our sample distribution.**

Let's revisit the example from the previous post. We've been asked to calculate the mean number of page views a website receives per day. We decided to assess this by taking a sample of the daily number of page views over a 30-day period:
```{r setup, include = FALSE, echo = FALSE}
library(knitr)
opts_chunk$set(dev = 'png')
```

```{r}
# Setting seed and generating a single sample
set.seed(567)
sample <- rpois(30, lambda = 220)
```

```{r bootstrap_plot_1, message=FALSE, echo=FALSE, fig.width = 10.5, fig.height = 4.5}
require(ggplot2); require(gridExtra)

# Set the colours for the graphs
barfill <- "#4271AE"
barlines <- "#1F3552"
line1 <- "black"
line2 <- "#FF3721"

# Plotting histogram of sample of daily page views
g1 <- ggplot(data=as.data.frame(sample), aes(sample)) + 
        geom_histogram(aes(y = ..density..), binwidth = 6, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Number of page views per day") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Sample distribution of page views over 30 days") + 
        theme(plot.title = element_text(lineheight=.8, face="bold"))

print(g1)
```

The mean of this sample is `r round(mean(sample), 1)`, which we will now treat as the population mean (without knowing the true population mean, we assume our sample mean is a very close approximation). 

A [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) would be the most likely population distribution, as we are measuring a count of events over time. However, we're not sure whether our sample of daily page views meets the assumptions of this distribution, and as such, we will bootstrap the SEM. To create a bootstrapped distribution of sample means, all we need to do is take repeated **resamples** from the distribution of our sample **with replacement** and take their means. The reason we must do this with replacement is because if we take a resample of 30 observations without replacement from our original sample, we will of course end up with the original sample for our resample! 

An important thing to note is that because we are relying on the sample to describe the population distribution (instead of using a known distribution such as the Poisson), we have to make sure our sample is a good representation of our population. I will cover choosing a representative sample in next week's blog post.

So let's bootstrap our SEM in R. We will take our sample of 30 days of page views and take 1,000 resamples from this with replacement.

```{r}
# Setting seed
set.seed(567)

# Generate the mean of each resample and store in a vector, and store each resample in a dataframe
mn_vector <- NULL
resample_frame <- data.frame(row.names = seq(from = 1, to = 30, by = 1))
for (i in 1 : 1000) {
    s <- sample(sample, 30, replace = TRUE)
    resample_frame <- cbind(resample_frame, s)
    mn_vector <- c(mn_vector, mean(s))
}

# Name the columns in the resample dataframe
names(resample_frame) <- paste0("n", seq(from = 1, to = 1000, by = 1))
```

Because we resample with replacement, each resampling gives a slightly different estimate of the mean. For example, the mean in resample 1 is `r round(mean(resample_frame$n1))`, and the mean from resample 2 is `r round(mean(resample_frame$n2))`.

```{r bootstrap_plot_2, message = FALSE, echo = FALSE, fig.width = 10.5, fig.height = 4.5}
mean1 <- data.frame(Means="Population mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(resample_frame$n1))
means <- rbind(mean1, mean2)

g1 <- ggplot(data=resample_frame, aes(resample_frame$n1)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 5, fill = barfill, colour = barlines) +
        xlab("Daily page views") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 1") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold")) +
        geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
         scale_color_manual(values=c("Population mean" = line1, "Sample mean" = line2))
           
mean1 <- data.frame(Means="Population mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(resample_frame$n2))
means <- rbind(mean1, mean2)
 
g2 <- ggplot(data=resample_frame, aes(resample_frame$n2)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 5, fill = barfill, colour = barlines) +
        xlab("Daily page views") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 2") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold")) +
        geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("Population mean" = line1, "Sample mean" = line2))

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

In the last blog post, I described how the mean of repeated samples from a population have a distribution that approximates the normal distribution. Similarly, the distribution of means of the resamples drawn from the sample also approximates the normal distribution.

```{r bootstrap_plot_3, echo = FALSE, fig.width = 10.5, fig.height = 4.5}
# Plotting a histogram with the +/- 1 and 2 standard error intervals.
sem1 <- data.frame(SEMs="+/- 1 SEM", 
                   vals = c(mean(mn_vector) - sd(mn_vector), mean(mn_vector) + sd(mn_vector)))
sem2 <- data.frame(SEMs="+/- 2 SEMs", 
                    vals = c(mean(mn_vector) - 2 * sd(mn_vector), mean(mn_vector) + 2 * sd(mn_vector)))
sems <- rbind(sem1, sem2)

g1 <- ggplot(data=as.data.frame(mn_vector), aes(mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each resample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Resamples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) +
        geom_vline(data=sems, aes(xintercept=vals, linetype = SEMs, 
                            colour = SEMs), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("+/- 1 SEM" = line1, 
                                    "+/- 2 SEMs" = line2))

print(g1)
```

Taking the mean of this distribution of means gives us the population mean (`r round(mean(mn_vector), 1)`), and taking the standard deviation of this distribution gives us the SEM (`r round(sd(mn_vector), 1)`). Moreover, because this distribution of resample means is normal, we know that &plusmn;1 standard errors around the mean represents 68% of the means of the resamples, and &plusmn;2 standard errors represents 95% of the resamples. In other words, 68% of the time when we take a resample we will end up with a mean between `r round(mean(mn_vector) - sd(mn_vector), 1)` and `r round(mean(mn_vector) + sd(mn_vector), 1)`, and 95% of the time when we take a resample we will end up with a mean between `r round(mean(mn_vector) - 1.96 * sd(mn_vector), 1)` and `r round(mean(mn_vector) + 1.96 * sd(mn_vector), 1)`, a pretty small margin of error around our population mean of `r round(mean(sample), 1)` page views per day.

## How do I know if it is worth bootstrapping my SEM?
Let's compare the result we get from just using the formula for calculating the SEM with the result we get from bootstrapping this sample. In the case of the Poisson distribution, this is $\sqrt{\lambda / n}$.

```{r, message = FALSE}
# Defining lambda and n
lambda <- mean(sample)
n <- 30

# Calculating SEM
sem <- sqrt(lambda / n)
```

We get a result of `r round(sem, 1)`, which is extremely close to our bootstrapped estimate (`r round(sd(mn_vector), 1)`). This is because the sample we took of daily page views met the assumptions of the Poisson distribution (given that it was drawn directly from it). In this case, bootstrapping our SEM does not improve our estimate of the SEM, therefore is not worth computing over simply using the SEM formula.

However, what if our population distribution did not fit a Poisson (or other) distribution? To demonstrate this, I have created a sample of 30 days of page views that is drawn from both the Poisson and uniform distributions.

```{r}
# Setting seed and drawing non-Poisson sample
set.seed(567)
sample_np <- c(rpois(10, lambda = 220), runif(20, min = 180, max = 245)) 

# Generate the mean of each resample and store in a vector
mn_vector_np <- NULL
for (i in 1 : 1000) {
    s <- sample(sample_np, 30, replace = TRUE)
    mn_vector_np <- c(mn_vector_np, mean(s))
}

# Generate bootstrapped SEM
b_sem <- sd(mn_vector_np)

# Generate formula-derived SEM
f_sem <- sqrt(mean(sample_np) / 30)
```

In this case, the SEM we obtain using bootstrapping is `r round(b_sem, 1)`, compared to `r round(f_sem, 1)` using the formula. We can see that the formula-based SEM slightly underestimates the amount of fuzziness in our estimate of the sample mean (as demonstrated for &plusmn;1 SEMs below). In this case we would be better off using a bootstrapped SEM rather than calculating it using the formula for the Poisson distribution.

```{r bootstrap_plot_4, echo = FALSE, fig.width = 10.5, fig.height = 4.5}
# Plotting a histogram with the +/- 1 and 2 standard error intervals.
sem1 <- data.frame(SEMs="+/- 1 SEM (bootstrap)", 
                   vals = c(mean(mn_vector_np) - b_sem, mean(mn_vector_np) + b_sem))
sem2 <- data.frame(SEMs="+/- 1 SEM (formula)", 
                    vals = c(mean(mn_vector_np) - f_sem, mean(mn_vector_np) + f_sem))
sems <- rbind(sem1, sem2)

g1 <- ggplot(data=as.data.frame(mn_vector_np), aes(mn_vector_np)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each resample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Resamples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) +
        geom_vline(data=sems, aes(xintercept=vals, linetype = SEMs, 
                            colour = SEMs), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("+/- 1 SEM (bootstrap)" = line1, 
                                    "+/- 1 SEM (formula)" = line2))

print(g1)
```

## The take away message
Bootstrapping is a great technique to use when you are not sure if your data meet the assumptions of the closest appropriate distribution. This is a really common problem outside of the nice, clean world of simulations, and bootstrapping offers us to way to gain more accurate statistics from messy, real world data. While it may be a cumbersome method to use with very large data (as resampling from millions of observations is very time consuming), it can offer a convenient way of checking the robustness of your data to the assumptions of your distributions and gaining more accurate estimates in such situations.

Some of the points and code in this blog post are adapted from the excellent [Statistical Inference](https://www.coursera.org/course/statinference) unit on Coursera by [Brian Caffo](https://twitter.com/bcaffo), [Jeff Leek](https://twitter.com/jtleek) and [Roger Peng](https://twitter.com/rdpeng).

Finally, the full code used to create the figures in this post is located in this [repo on my Github page](https://github.com/t-redactyl/Blog-posts).
