---
title: "A Gentle Introduction to Bootstrapping"
output: html_document
---

In the previous post, I explained the general principles behind the **standard error of the mean** (or **SEM**). The idea underlying the SEM is that you take repeated samples from the population of interest, and take the standard deviation of the means of these samples. Formulas for the SEM are available for each distribution, and accurately reflect the degree of error in your sample mean _if your data meets the assumptions of that distribution._

## What if I'm not sure if my data meets the assumptions of a distribution?

You might have a case where the true population distribution of your sample doesn't meet the assumptions of the closest appropriate distribution. For example, you might be interested in describing the mean levels of depression in a population, which is naturally positively skewed (i.e., most people in a population have no or low scores on measures of depression):

```{r, message=FALSE}
require(fGarch)
set.seed(567)
depression <- rsnorm(100, mean = 4, sd = 1, xi = 3)
```

```{r, message=FALSE, echo=FALSE}
require(ggplot2); require(gridExtra)

barfill <- "#6baed6"
barlines <- "#2171b5"
line1 <- "black"
line2 <- "#FF0000"

g1 <- ggplot(data=as.data.frame(depression), aes(depression)) + 
        geom_histogram(aes(y = ..density..), binwidth = .5, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Depression scores") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Scores on a Depression Scale") + 
        theme(plot.title = element_text(lineheight=.8, face="bold"))
        
print(g1)
```
    
In this case, using the SEM formula for the normal distribution would be inappropriate due to this skewness. Instead, you can create empirically derived SEMs using a technique called **bootstrapping.**
    
## What is bootstrapping?

Bootstrapping involves basically the same principle as calculating the SEM I described in the last post, except instead of sampling from the _population distribution_, we resample from our _sample distribution._

Let's revisit the example from the previous post. We've been asked to calculate the mean number of page views a website receives per day. We decided to record the daily number of page views over a 30-day period:

```{r}
# Setting seed and generating a single sample
set.seed(567)
sample <- rpois(30, lambda = 220)
```

```{r, message=FALSE, echo=FALSE}
# Plotting histogram of sample of daily page views
g1 <- ggplot(data=as.data.frame(sample), aes(sample)) + 
        geom_histogram(aes(y = ..density..), binwidth = 6, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Number of page views per day") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Sample distribution of page views over 30 days") + 
        theme(plot.title = element_text(lineheight=.8, face="bold"))

print(g1)
```

Although a Poisson distribution would be the most likely population distribution, we've decided we're not sure whether our sample of daily page views meets the assumptions of this distribution. As such, we will bootstrap the SEM. To create a bootstrapped distribution of sample means, all we need to do is take repeated resamples from the distribution of our sample _with replacement_ and take their means. The reason we must do this with replacement is because if we have a sample distribution of size, say 30, and we take a resample from this of size 30, we will just end up with the original sample for our resample! 

An important thing to note is that because we are relying on the sample to describe the population distribution (instead of using a known distribution such as the Poisson), we have to make sure our sample is a good representation of our population. I will cover choosing a representative sample in next week's blog post.

So let's bootstrap our SEM in R. We will take our sample of 30 days of page views and take 1,000 resamples from this with replacement.

```{r}
# Setting seed
set.seed(567)

# Generate the mean of each resample and store in a vector, and store each resample in a dataframe
mn_vector <- NULL
resample_frame <- data.frame(row.names = seq(from = 1, to = 30, by = 1))
for (i in 1 : 1000) {
    s <- sample(sample, 30, replace = TRUE)
    resample_frame <- cbind(resample_frame, s)
    mn_vector <- c(mn_vector, mean(s))
}

# Name the columns in the resample dataframe
names(resample_frame) <- paste0("n", seq(from = 1, to = 1000, by = 1))
```

You can see we get variability in our estimate of the population mean from resample to resample. For example, the mean in resample 1 is `r round(mean(resample_frame$n1))`, and the mean from resample 2 is `r round(mean(resample_frame$n2))`.

```{r, message = FALSE, echo = FALSE, fig.width = 15}
mean1 <- data.frame(Means="Population mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(resample_frame$n1))
means <- rbind(mean1, mean2)

g1 <- ggplot(data=resample_frame, aes(resample_frame$n1)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 5, fill = barfill, colour = barlines) +
        xlab("Daily page views") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 1") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold")) +
        geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
         scale_color_manual(values=c("Population mean" = line1, "Sample mean" = line2))
           
mean1 <- data.frame(Means="Population mean", vals = 220)
mean2 <- data.frame(Means="Sample mean", vals = mean(resample_frame$n2))
means <- rbind(mean1, mean2)
 
g2 <- ggplot(data=resample_frame, aes(resample_frame$n2)) + 
        geom_histogram(aes(y = ..density..), 
                       binwidth = 5, fill = barfill, colour = barlines) +
        xlab("Daily page views") +
        ylab("Density") +
        theme_bw() +
        ggtitle("Sample 2") + 
        theme(plot.title = element_text(lineheight=1.1, face="bold")) +
        geom_vline(data=means, aes(xintercept=vals, linetype = Means, 
                             colour = Means), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("Population mean" = line1, "Sample mean" = line2))

grid.arrange(g1, g2, nrow = 1, ncol = 2)
```

In the last blog post, I described how the mean of repeated samples from a population have a distribution that approximates the normal distribution. Similarly, the distribution of means of the resamples drawn from the sample also approximate the normal distribution. Taking the standard deviation of this sample gives us the SEM (`r sd(mn_vector)`).

```{r, echo = FALSE, fig.width=15}
# Plotting a histogram with the +/- 1 and 2 standard error intervals.
sem1 <- data.frame(SEMs="+/- 1 SEM", 
                   vals = c(mean(mn_vector) - sd(mn_vector), mean(mn_vector) + sd(mn_vector)))
sem2 <- data.frame(SEMs="+/- 2 SEMs", 
                    vals = c(mean(mn_vector) - 2 * sd(mn_vector), mean(mn_vector) + 2 * sd(mn_vector)))
sems <- rbind(sem1, sem2)

g1 <- ggplot(data=as.data.frame(mn_vector), aes(mn_vector)) + 
        geom_histogram(aes(y = ..density..), binwidth = 1, 
                 col = barlines, 
                 fill = barfill) + 
        xlab("Mean of each resample") + 
        ylab("Density") + 
        theme_bw() + 
        ggtitle("Distribution of Means of 1,000 Resamples") + 
        theme(plot.title = element_text(lineheight=.8, face="bold")) +
        geom_vline(data=sems, aes(xintercept=vals, linetype = SEMs, 
                            colour = SEMs), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("+/- 1 SEM" = line1, 
                                    "+/- 2 SEMs" = line2))

print(g1)
```

In our case, taking a sample of 30 days gives us a pretty accurate assessment of the population mean, with 68% of our samples giving a mean between `r round(mean(mn_vector) - sd(mn_vector), 2)` and `r round(mean(mn_vector) + sd(mn_vector), 2)`, and 95% of our samples giving a mean between `r round(mean(mn_vector) - 1.96 * sd(mn_vector), 2)` and `r round(mean(mn_vector) + 1.96 * sd(mn_vector), 2)`. In other words, 68% of the time when we take a sample we will end up with a mean between `r round(mean(mn_vector) - sd(mn_vector), 2)` and `r round(mean(mn_vector) + sd(mn_vector), 2)`, and 95% of the time when we take a sample we will end up with a mean between `r round(mean(mn_vector) - 1.96 * sd(mn_vector), 2)` and `r round(mean(mn_vector) + 1.96 * sd(mn_vector), 2)`. This is a pretty tight band around our population mean of 220 page views per day, indicating that a sample of 30 gives a pretty good estimate of the mean.

## How do I know if it is worth bootstrapping?

You may have also noticed that the bootstrapped SEM using 1,000 resamplings was `r round(sd(mn_vector), 2)`, which is extremely close to the SEM derived from sampling from the population () in the last blog post. This is because the sample we took of daily page views met the assumptions of the Poisson distribution (given that it was drawn directly from it).



* Modify a Poisson distribution so it no longer meets the assumptions and bootstrap it - compare to SEM from Poisson distribution and also formula. (Probably add outliers.)


```{r}
# Setting seed
set.seed(567)
sample_np <- c(rpois(10, lambda = 220), runif(20, min = 180, max = 245)) 

# Generate the mean of each resample and store in a vector, and store each resample in a dataframe
mn_vector_np <- NULL
for (i in 1 : 1000) {
    s <- sample(sample_np, 30, replace = TRUE)
    mn_vector_np <- c(mn_vector_np, mean(s))
}

mean(sample_np)
sqrt(mean(sample_np) / 30)
sd(mn_vector_np)

```



## How do I know if my sample represents my population?

Choosing a good sample is less about number crunching and more about study design. Depending on the population of interest, it can be a bit tricky. Important elements of representative sampling include:  
1. **Sample size:** The trick to choosing a large enough sample size is making sure you gain sufficient information about the patterns you are trying to describe. For example, if we had only sampled 10 days, would that have been too small to avoid sampling extreme values by chance that threw off our estimate of the mean? If the event we were trying to sample was very rare (e.g., 1 event per 100 days), would sampling 30 days be long enough to assess its mean occurrence? However, the opposite consideration often occurs with sample size, where taking samples that are much larger than is needed is also undesirable as there may be time, monetary or even ethical reasons to limit the number of observations collected. For example, the company that asked us to assess page views would likely be unhappy if we spent 100 days collecting information on mean daily page views when the same question could be reliably answered from 30 days of data collection. There is a whole collection of methods that help you decide on the correct sample size (called **power**) which are beyond the scope of this already long blog post.
2. **Representativeness:** Another consideration is that we must make the data we collect a good reflection of the population we are trying to describe. Say that I tried to collect my data after a new campaign was launched advertising the website. This would not be a representative sample of the usual mean page views per day as page views are likely to be higher at that time.
3. **Measurement:** A final step in gaining a representative sample is remembering there may be a gap between a _concept_ of interest and the _method_ you need to use to measure it. In our example, there is likely to be little gap between what we are trying to measure (number of page views) and the method (a counter of page visitors). However, what if we don't actually care about page views per say, but the popularity of a website? Would it be reasonable to say that websites with higher mean page views have greater popularity?
