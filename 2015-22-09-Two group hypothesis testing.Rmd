---
title: Two-Group Hypothesis Testing: Independent Samples T-Tests
date: 2015-09-08
comments: false
tags: Statistics, R, Data Simulations, Hypothesis Testing
keywords: rlanguage, t test, simulations, data science, hypothesis testing
---

In some of my previous posts, I asked you to imagine that we work for a retail website that sells children's toys. In the past, they've asked us to estimate the mean number of page views per day (see here and here for my posts discussing this problem). Now, they've launched two online advertising campaigns, and they want us to see if these campaigns are equally effective or one is better than the other.

The way we assess this is a central method in statistical inference called **hypothesis testing**. The usual workflow of hypothesis testing is as follows:  
1. Define your question;   
2. Define your hypotheses;     
3. Pick the most likely appropriate test distribution (t, z, binomial, etc.);  
4. Compute your test statistic;  
5. Work out whether we reject the null hypothesis based whether your test statistic exceeds a critical statistic under this distribution.

This blog post will through each of these steps in detail, using our advertising campaign problem as an example.

## Defining your question
The first, and most important step to any analysis is working out _what you are asking_ and _how you will measure it_. A reasonable way to assess whether the advertising campaigns are equally effective or not could be to take all site visits that originate from each of the campaigns and see how much money the company makes from these visits (i.e., the amount of toys the customers who visit buy). A way we could then test if the amount generated differs is to take the mean amount of money made from each campaign and statistically test whether these means are different.

## Defining your hypotheses
The next step is defining so we can test these questions statistically. When you define hypotheses, you are trying to compare two possible outcomes. The first is the **null hypothesis** (H$\sf{_{0}}$), which represents the "status quo" and is assumed to be correct until statistical evidence is presented that allows us to reject it. In this case, the null hypothesis is that there is no difference between the mean amount of income generated by each campaign. Stating this more formally, we get:

H$\sf{_{0}}$: $\mu\sf{_{1}}$ = $\mu\sf{_{2}}$, or $\mu\sf{_{1}}$ - $\mu\sf{_{2}}$ = 0
where $\mu\sf{_{1}}$ is the mean of the first population, and $\mu\sf{_{2}}$ is the mean of the second population.

The **alternative** hypothesis (H$\sf{_{a}}$) is that there is a difference between the mean level of incomes generated by each campaign. More formally, the alternative hypothesis is:
H$\sf{_{a}}$: $\mu\sf{_{1}}$ &#8800; $\mu\sf{_{2}}$, or $\mu\sf{_{1}}$ - $\mu\sf{_{2}}$ &#8800; 0

In other words, we are trying to test whether the difference in the mean levels of income generated by each campaign is sufficiently different from 0 (no difference).

## Picking the most appropriate distribution
The most appropriate distribution for our test depends on what we _assume the population distribution is_. As the next step in our study of which campaign is correct, we took representative samples of site visits originating from each campaign and recorded how much was purchased (simulated below):

```{r}
set.seed(567)
campaign.1 <- rt(40, 39) * 60 + 310
campaign.2 <- rt(40, 39) * 58 + 270
```

When we look at the data, it appears approximately normal. However, due to time pressure, we were only able to get 40 site visits from each campaign. As such, we'll assume we should use a t-distribution, which performs better with "normally-shaped" data which have small sample sizes. According to our samples, the first advertising campaign generated a mean of $`r round(mean(campaign.1), 2)` per visit with a standard deviation of $`r round(sd(campaign.1), 2)`, and the second campaign generated a mean of $`r round(mean(campaign.2), 2)` per visit with a standard deviation of $`r round(sd(campaign.2), 2)`.

(**Technical aside:** The reason that the t-distribution performs better than the normal distribution under small samples is because we use the sample standard deviation in our calculation of both the t- and z-distributions, rather than the true population standard deviation. At large samples, the sample standard deviation is expected to be a very close approximation to the population standard deviation; however, this is not the case in smaller samples. As such, using a z-distribution for small samples leads to an underestimation of the standard error of the mean and consequently, confidence intervals. Incidently, this also means that as you collect more and more data, the t-distribution behaves more and more like the z-distribution, meaning that it is a safe bet to use the t-distribution if you are not sure if your sample is big "enough".)

## Computing your test statistic
The next step is to get some measure of whether these values are different. When talking about hypothesis tests, I pointed out that the null hypothesis can be reframed as $\mu\sf{_{1}}$ - $\mu\sf{_{2}}$ = 0, and the alternative hypothesis as $\mu\sf{_{1}}$ - $\mu\sf{_{2}}$ &#8800; 0. As such, we can test our hypotheses by taking the difference of our two campaigns as the **difference between the two means**.

```{r}
diff.means <- mean(campaign.1) - mean(campaign.2)
```

This gives us a difference of $`r round(diff.means, 2)`. In order to test whether this difference is statistically meaningful, we need to convert either the mean or the distribution we are comparing it to into the same units. The approach I will take here is to make the mean difference comparable to a standardised t-distribution, which is where units of the mean difference are converted from dollars to numbers of standard deviations from the mean. In this standardised t-distribution, the mean is 0, and the standard deviation units are symmetrically negative (below the mean) or positive (above the mean).

The standardised mean difference is called the **test statistic**, and in this case is specifically called a **t-value.**

```{r}
# First calculate the pooled standard deviation (assuming equal variances, with equal sample sizes)
sp <- sqrt((sd(campaign.1)^2 + sd(campaign.2)^2)/2)

# Then calculate the standard error of the mean difference
se <- sp * (1 / length(campaign.1) + 1 / length(campaign.1))^.5

# The t test statistic is the difference in means divided by the standard error
t.test.stat <- diff.means / se
```

## Rejecting or accepting the null hypothesis
The final step is working out whether our test statistic is "different enough" from 0 (or no difference) is to work out what the **confidence interval** around 0 is. This is determined by two things.

### Confidence levels and rejection regions
The cutoff for deciding whether our test statistic is meaningful is determined by the **confidence interval** around our mean. Anything beyond the confidence interval is called the **rejection region**, because if our test statistic falls into it, we reject the null hypothesis. The first thing to work out is the level of confidence we want to have (or what **Type I error rate** we are willing to accept). The typical benchmark is 95% confidence, or a Type I error rate of 5%.

The way the rejection region is distributed is determined by whether we use a **one-sided** or a **two-sided** test. One-sided tests are appropriate when you are trying to see whether one value is bigger (i.e., $\mu\sf{_{1}}$ - $\mu\sf{_{2}}$ > 0) or smaller than another ($\mu\sf{_{1}}$ - $\mu\sf{_{2}}$ < 0). For example, if we were trying to test whether campaign 1 brought in more revenue than campaign 2, it would be appropriate for us to use a one-sided test. In the case of the one-sided test, the rejection region lies in one tail either above or below the mean. 

As we are looking for any difference (whether positive or negative difference between the two), we must use a two sided test. In a two-sided test, the rejection region lies in both tails of the test distribution. However, as the total rejection region must add up to 5%, we divide it so that each tail has 2.5% of the distribution beyond the confidence interval. You can see this below, where the one-sided rejection region is close to the mean than the two-sided. The implication of this is that we need less extreme test statistics to achieve significance in a one-sided test than in a two-sided, therefore it is very bad practice to incorrectly use a one-sided test when a two-sided test would be appropriate.

```{r rejection_region_plots, echo = FALSE, message = FALSE, fig.width = 10.5, fig.height = 6.5}
require(ggplot2); require(gridExtra)

col1 <- "black"
col2 <- "#FF3721"

one.sided.rr <- qt(.95, 78)
line1 <- data.frame(Values="Rejection region (one-sided)", vals = one.sided.rr)
line2 <- data.frame(Values="Mean", vals = 0)
lines <- rbind(line1, line2)

g1 <- ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
        stat_function(fun = dt, args = list(df = 28)) +
        xlab("Standardised difference in mean income") + 
        ylab("Density") + 
        theme_bw() +
        geom_vline(data=lines, aes(xintercept=vals, linetype = Values, 
                            colour = Values), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("Rejection region (one-sided)" = col1, 
                                    "Mean" = col2))

two.sided.rr <- qt(c(.025, .975), 78)
line1 <- data.frame(Values="Rejection region (two-sided)", vals = two.sided.rr)
line2 <- data.frame(Values="Mean", vals = 0)
lines <- rbind(line1, line2)

g2 <- ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
        stat_function(fun = dt, args = list(df = 28)) +
        xlab("Standardised difference in mean income") + 
        ylab("Density") + 
        theme_bw() +
        geom_vline(data=lines, aes(xintercept=vals, linetype = Values, 
                            colour = Values), size = 1, show_guide = TRUE) + 
        scale_color_manual(values=c("Rejection region (two-sided)" = col1, 
                                    "Mean" = col2))

grid.arrange(g1, g2, nrow = 2, ncol = 1)
```

### Degrees of freedom
Secondly, because we are using the t distribution, our specific confidence levels are determined by the **degrees of freedom** of the sample, which is given by the pooled sample size minus 2 (so in our case we have 40 + 40 - 2 = 78 degrees of freedom).

### Let's carry out this damn test!
After all my rambling, let's see whether our standardised mean difference is actually in the rejection region.

```{r}
# Generate the 95% confidence interval.
lci <- -1 * qt(c(.975), 78)
uci <- qt(c(.975), 78)
```

```{r t_test_plot, echo = FALSE, message = FALSE, fig.width = 10.5, fig.height = 4.5}
# Plot the t function with test statistic and relevant 95% confidence interval
require(ggplot2)

line1 <- data.frame(Values="95% CI of no difference", vals = c(lci, uci))
line2 <- data.frame(Values="Test statistic", vals = t.test.stat)
lines <- rbind(line1, line2)

ggplot(data.frame(x = c(-4, 4)), aes(x)) + 
    stat_function(fun = dt, args = list(df = 28)) +
    xlab("Standardised difference in mean income") +  
    ylab("Density") + 
    theme_bw() +
    geom_vline(data=lines, aes(xintercept=vals, linetype = Values, 
                        colour = Values), size = 1, show_guide = TRUE) + 
    scale_color_manual(values=c("95% CI of no difference" = col1, 
                                "Test statistic" = col2))
```

As you can see below, it is, therefore the difference in the outcomes generated between the two campaigns is **significant at the 0.05 level**.

## Let's repeat in R!
Now I've shown you how a t-test works under the hood, let's go through how you would **actually** do this in your day-to-day use of R.

All we need to do is call the t.test function, like so:
```{r}
test <- t.test(campaign.1, campaign.2, paired = FALSE, var.equal = TRUE)
test 
```

As you can see, the results are the same. The t-value (or test statistic) is `r round(test$statistic, 3)` and our p-value is `r round(test$p.value, 3)`, which is less than 0.05.

## Statistical versus practical significance
In the end, we found out that the two advertising campaigns generated significantly different amounts of revenue per visit, with campaign 1 generating a mean of $`r round(diff.means, 2)` more per visit than campaign 2. However, does this automatically mean we go with campaign 1? For example, what if the reason that campaign 1 is generating more income is because the ads are being placed on high-end websites, where the sort of people seeing the ad have more disposible income than the general population, but the ad costs are much more expensive. Is the revenue difference big enough to justify the increased cost of advertising on this platform?



